---
title: "BOO2024 - Hands-on workshop DEG analysis"
author: ""
date: "`r format(Sys.time(), '%d %B %Y')`"
output:
  html_document:
    theme: default
    highlight: kate
    toc: true
    toc_float:
      collapsed: false
      smooth_scroll: true
---

# Setup {.tabset}
```{r include=FALSE, echo=TRUE, message=FALSE}
rm(list = ls()); gc()
knitr::opts_chunk$set(tidy.opts = list(width.cutoff = 80), tidy = TRUE)
```


## Load packages
### CRAN
```{r}
# Check if pacman is available and install
if(!require("pacman", quietly = T)){install.packages("pacman")}; library(pacman)

# use packman to install CRAN packages
p_load(tidyverse, ggpubr, corrr, ggfortify, ggcorrplot, ggdendro, data.table, GGally)
```


## Set directories
```{r}
# input directory
if(!dir.exists("INPUT")){
  dir.create(path = file.path(getwd(), "DATA"))
}
input_dir <- file.path(getwd(), "DATA")

# output directory
if(!dir.exists("OUTPUT")){
  dir.create(path = file.path(getwd(), "INPUT"))
}
output_dir <- file.path(getwd(), "INPUT")

# plot directory
if(!dir.exists("PLOT")){
  dir.create(path = file.path(getwd(), "PLOT"))
}
plot_dir <- file.path(getwd(), "PLOT")
```


## Load functions
```{r}
# Function: Get low cpm probes ----
get_low_cpm_probes <- function(countdata, metadata, exclude){

  if(!has_rownames(countdata)){
    countdata <- countdata %>%
      column_to_rownames(var = names(countdata %>% dplyr::select(where(is.character))))
  }

  if(!all(c("SAMPLE_ID", "MEAN_ID") %in% colnames(metadata))){
    stop("Metadata must contain columns SAMPLE_ID and MEAN_ID")
  }

  countdata <- countdata %>% select(-contains(paste(c(exclude, collapse = "|"))))

  countdata <- data.frame(ifelse(test = countdata >= 1, yes = 1, no = 0)) %>%
    mutate(across(where(is.numeric), ~as.logical(.x)))

  countdata <- countdata %>%
    rownames_to_column(var = "GENE_SYMBOL") %>%
    pivot_longer(cols = where(is.logical), names_to = "SAMPLE_ID") %>%
    left_join(x = metadata %>%
                dplyr::select(SAMPLE_ID, MEAN_ID) %>%
                group_by(MEAN_ID) %>%
                mutate(n = n()) %>%
                ungroup(),
              by = "SAMPLE_ID") %>%
    group_by(MEAN_ID, n, GENE_SYMBOL) %>%
    summarise(value = sum(value), .groups = "drop") %>%
    filter(value <= n * 0.75)

  n_mean_id <- length(unique(countdata$MEAN_ID))

  countdata %>%
    group_by(GENE_SYMBOL) %>%
    count() %>%
    filter(n == n_mean_id) %>%
    pull(GENE_SYMBOL) %>%
    unique()
}

```


# Load data {.tabset}

## Metadata
Here you should load the metadata you obtained.
* What is metadata?
 The data with the characteristics of the samples
```{r}
metadata <- fread(input = file.path(input_dir, "EUT082_RPTECTERT1_cyclosporin_metadata.csv"))
```

## Countdata
Here you should load the raw data you obtained.
* What is (raw) count data?
 The data with the amount of reads aligned to each gene
```{r}
countdata_raw <- fread(input = file.path(input_dir, "EUT082_RPTECTERT1_cyclosporin_rawcounts.csv"))
```

## Wrangle countdata and metadata
Inspect the metadata object by clicking on it in the environment panel in the top right and answer the following questions.
*	How many samples are in the data? (hint: metadata$SAMPLE_ID)
 197 samples
*	What cell type was used for compound exposure?*
 RPTEC.TERT1
*	Which time points are included?
 4, 8, 16, 24, 48 & 72
*	Which concentrations were used for the exposure?
 0, 0.02, 0.10, 0.50, 1.00, 2.00, 5.00, 10.00, 20.00, 40.00
*	Which compounds do we consider "treatment" and "control"?
 CSA is treatment and DMSO is control
*	Treatment conditions are a combination of treatment, dose and time. What treatment conditions are in the data? (hint: unique(expand(metadata, nesting(COMPOUND, CONCENTRATION, TIMEPOINT))) 
 Every dose has each timepoint for CSA except 0 and DMSO has only 0 for every timepoint

Now in inspect the raw countdata object by clicking it in the enviroment panel in the top right and answer the following question.
*	How many probes are in the data? (hint: countdata_raw$GENE_SYMBOL)
22537
*	Look at the dimensions of the dataframe (rows and columns). How many probes (rows) were measured?
22537
```{r error=F,warning=F,message=F}
# We wrangle the original metadata to generate new treatment conditions and format the metadata into a clear overview. Have a look!
metadata <- metadata %>% 
  unite(col = "MEAN_ID", c(CELL_ID, COMPOUND_ABBR, TIMEPOINT, DOSE), remove = F) %>% 
  select(SAMPLE_ID, MEAN_ID, TIME, TIMEPOINT, REPLICATE, COMPOUND, COMPOUND_ABBR, CELL_ID, SPECIES, DOSE, DOSE_LEVEL) %>%
  rename("CONCENTRATION" = DOSE)

# We rename the countdata column with the probes and reorder all other columns to match the metadata sample id order.  
countdata_raw <- countdata_raw %>% 
  rename(GENE_SYMBOL = PROBE_ID) %>%
  select(GENE_SYMBOL, metadata$SAMPLE_ID) # Reorder columns

# We print the output
 { print("Raw countdata")
  cat("\n")
  countdata_raw %>% str()
  cat("\n")
  print("Metadata")
  cat("\n")
  metadata %>% str()}

```


## QC1: Total read count filter
The total read counts filter, also called sample size filter, is applied to discart samples with a low library size. Answer the following questions.
*	What is the definition of library size?
 The total number of reads that were sequenced of each sample
*	What is the definition of low library size samples?
 Samples that are unwanted under the threshold of a million reads and need to be removed
*	How many samples are excluded from further analysis â€“ if there are any? (Hint: look at the plot)
 0
*	Why do we need to eliminate the low library size samples before normalizing the data?
 Because it will incorrectly form the data to those outliers, you dont want a too high variance
```{r error=F,warning=F,message=F}
# We set the threshold to 1 million
countdata_threshold <- 1E6


# We take the sum of every individual column and transpose the data frame
size <- countdata_raw %>%
  summarise(across(where(is.numeric), sum)) %>%
  pivot_longer(cols = everything(), names_to = "SAMPLE_ID", values_to = "SAMPLE_SIZE")


# We make a bar plot using ggplot of the sample sizes with the threshold as red horizontal line for quick interpretation
ggplot(data = size, mapping = aes(x = SAMPLE_ID, y = SAMPLE_SIZE)) +
  geom_bar(stat = "identity") +
  theme_bw() +
  theme(axis.text.x = element_text(angle=90, vjust=0.5, hjust=1, size = 2)) +
  geom_hline(yintercept=countdata_threshold, size = 2, color = "red")+
  ggtitle("Sample size of raw countdata") + 
  ylab('Sample size')


# We identify the samples with a size (total amount of counts) below or equal to the threshold.
bad_samples = size %>% filter(SAMPLE_SIZE <= countdata_threshold)

# We filter the raw countdata for the bad samples, "fsample" in countdata_raw_fsample means filtered sample
countdata_raw_fsample = countdata_raw %>% select(-all_of(bad_samples %>% pull(SAMPLE_ID)))

# We filter the metadata for the bad samples, "fsample" in metadata_fsample means filtered sample
metadata_fsample = metadata %>% filter(!SAMPLE_ID %in% bad_samples$SAMPLE_ID)

# We print the output
  bad_samples %>% str()  
```


## QC2: Relevance filter at the CPM level

#### QC2.1: Relevance filter to be applied to normalized data: count per million normalization formula
```{r}
# CPM (Counts Per Million) are obtained by dividing counts by the library counts sum and multiplying the results by a million. 
cpm_normalization <- function(x){
(x/sum(x))*1000000
}

countdata_cpm <- data.frame(apply(countdata_raw %>% column_to_rownames(var = "GENE_SYMBOL"), 2, cpm_normalization))
```


#### QC2.2: Relevance filter
The relevance filter is applied to discart all probes that do not reach at least 1 CPM in all 3 replicates across all treatment conditions. Answer the following questions.
* What is the definition of probes?
 Probes are stretches of a single-stranded RNA
* How do we identify if a probe has low counts?
 When the probe has no reads (0) in every sample
* How many low count probes (probes that are exempted from analysis) are in the data?
 8913
* Why do we need to elimiated the low expressed probes?
 They will interfere in the normalization
```{r error=F,warning=F,message=F}

low_cpm_probes <- get_low_cpm_probes(countdata = countdata_cpm, metadata = metadata, exclude = c())
countdata_raw_fsample_fprobe = countdata_raw_fsample %>% filter(!GENE_SYMBOL %in% low_cpm_probes)

  low_cpm_probes %>% str() 
```

```{r}
options(max.print = 9000)
print(low_cpm_probes)
```



## QC3: Sum the raw counts of probes targeting the same gene 
* Why are there multiple probes for a single gene?
 Some RNA strands are very long so they are done in parts
* Why do we take the sum of the probes targeting the same gene and not the mean?
 Because they are parts of 1 RNA strand and the mean would mean its 3 different ones

In the 'probe_distribution' we included only the gene name with the highest probe count out of all genes. 
* Why does this gene have many probes? (Hint: use external resources such as NCBI gene, GeneCards or UniProt)
 Because the gene is very long
*  What are the differences in data frame dimension (rows and columns) before and after summing the probes?
 There are less rows, because some probes are now summed
data frame dimension (rows and columns) before and after summing the probes?
```{r error=F,warning=F,message=F}
# After filtering for low cpm probes how many probes are left that target multiple genes
probe_distribution <- countdata_raw_fsample_fprobe %>% 
  separate(col = GENE_SYMBOL, into = c("GENE_SYMBOL", "PROBE"), sep = "_") %>% 
  select(GENE_SYMBOL, PROBE) %>% 
  group_by(GENE_SYMBOL) %>% 
  summarise(x = n()) %>% 
  count(x) %>% select("Probe count" = x,
                      "Unique genes" = n)

# We attach the gene symbol for the highest probe count only 
probe_distribution <- countdata_raw_fsample_fprobe %>% 
  separate(col = GENE_SYMBOL, into = c("GENE_SYMBOL", "PROBE"), sep = "_") %>% 
  select(GENE_SYMBOL, PROBE) %>% 
  group_by(GENE_SYMBOL) %>% 
  count() %>% 
  arrange(desc(n)) %>% 
  filter(n == 9) %>% # Change '9'to the highest 'Probe count' in the probe_distribution dataframe
  right_join(y = probe_distribution, by = c("n" = "Probe count")) %>% 
  arrange(n) %>% 
  select("Probe Count" = n, `Unique genes`, GENE_SYMBOL)

# We sum the probes targeting the same gene
countdata_raw_fsample_fprobe_sumprobe <- countdata_raw_fsample_fprobe %>% 
  separate(col = GENE_SYMBOL, into = c("GENE_SYMBOL", "PROBE"), sep = "_") %>% 
  group_by(GENE_SYMBOL) %>% 
  summarise(across(where(is.numeric), sum), .groups = "drop")

# We print the output
{  print(probe_distribution)
  cat("\n")
  print("Dataframe dimensions before probe sum")
  dim(countdata_raw_fsample_fprobe) %>% str()
  cat("\n")
  print("Dimensions after probe sum")
  dim(countdata_raw_fsample_fprobe_sumprobe) %>% str()
}
```


## Countdata CPM normalization
* Why do we need to normalize the counts for further downstream analysis?
 So analysis becomes easier and because the data will be standardized
* What is the formula for CPM normalization?
 number of reads mapped to the gene/total number of mapped genesx1000000
* What is the main difference between the dataframes before and after CPM normalization, and can you explain the difference?
 It goes from integer to numeric. Numeric has decimals.
```{r error=F,warning=F,message=F}
# We use the apply function to apply our cpm_normalization column wise (indicated by the 2) over the countdata_raw_fsample_fprobe_sumprobe object
countdata_cpm_fsample_fprobe_sumprobe <- data.frame(apply(countdata_raw_fsample_fprobe_sumprobe %>% 
                                                            column_to_rownames(var = "GENE_SYMBOL"), 2, cpm_normalization)) %>% 
  rownames_to_column("GENE_SYMBOL")

# We print the output
{  print("Countdata raw")
  cat("\n")
  data.frame(countdata_raw_fsample_fprobe_sumprobe %>% column_to_rownames(var = "GENE_SYMBOL") %>% str())
  cat("\n")
  print("Countdata cpm normalized")
  cat("\n")
  countdata_cpm_fsample_fprobe_sumprobe %>% str()
} 
```

# Counts distribution 
We can make distribution plots to visualize the difference between the raw countdata and normalized countdata. Look at the two counts distributions and answer the following questions.
* What is the difference between the distribution of the raw counts and the CPM normalized counts?
 The shape is more a line so you can see it better
*	Based on the distribution plots and identifying the main differences between the dataframes before and after CPM normalization, why do we need to normalize the counts for further analysis?
 It is to see if we can take out bad samples under the mean
```{r}
# Reshape raw countdata to long format. Have a look to see the change!
countdata_raw_long <- countdata_raw_fsample_fprobe_sumprobe %>%
  pivot_longer(cols = -GENE_SYMBOL, names_to = "SAMPLE_ID", values_to = "COUNTS")

# Reshape CPM normalized countdata to long format. Have a look to see the change!
countdata_cpm_long <- countdata_cpm_fsample_fprobe_sumprobe %>%
  # rownames_to_column(var = "GENE_SYMBOL") %>%
  pivot_longer(cols = -GENE_SYMBOL, names_to = "SAMPLE_ID", values_to = "COUNTS")


# count distribution from the raw count data
ggplot(countdata_raw_long, aes(x = reorder(SAMPLE_ID, COUNTS, FUN = median), y = COUNTS+1)) +
        geom_boxplot(size = 0.3, outlier.size = 0.5) +
        scale_y_log10(limits = c(1, max(countdata_raw_long$COUNTS))) +
        theme_classic() +
        theme(plot.title = element_text(size=14, face="bold", vjust = 2, hjust = 0.5), 
              axis.title.x = element_text(size=12, vjust = 0.25),
              axis.title.y = element_text(size=12, vjust = 1),
              axis.text.x = element_text(size=8, angle=90, vjust=0.5, hjust=1),
              axis.text.y = element_text(size=12)) +
        ggtitle("Distribution raw counts") + ylab('counts') + xlab("sampleID")

# count distribution from the CPM normalized count data
ggplot(countdata_cpm_long, aes(x = reorder(SAMPLE_ID, COUNTS, FUN = median), y = COUNTS)) +
        geom_boxplot(size = 0.3, outlier.size = 0.5) +
  scale_y_log10(limits = c(1, max(countdata_cpm_long$COUNTS))) +
        theme_classic() +
        theme(plot.title = element_text(size=14, face="bold", vjust = 2, hjust = 0.5), 
              axis.title.x = element_text(size=12, vjust = 0.25),
              axis.title.y = element_text(size=12, vjust = 1),
              axis.text.x = element_text(size=8, angle=90, vjust=0.5, hjust=1),
              axis.text.y = element_text(size=12)) +
        ggtitle("Distribution CPM Normalized counts") + ylab('CPM Normalized counts') + xlab("sampleID")

```


## PCA plot and correlation plot 

### Principal component analysis on CPM normalized counts
We make a PCA plot to help detect outliers that behave differently from the majority of samples. 
* Why do we use the CPM normalized countdata and not the raw countdata?
 It ensures that each attribute has the same level of contribution, preventing one variable from dominating others
* What conclusions can you draw from inspection of the PCA plot?
 0 outliers have been detected
```{r error=F,warning=F,message=F}
# We transpose the prepared count data: sampleIDs from the column names to a single row, and all GENE_SYMBOL count data to an individual column
pca_data <- countdata_cpm_fsample_fprobe_sumprobe %>% 
  # rownames_to_column(var = "GENE_SYMBOL") %>% 
  pivot_longer(-GENE_SYMBOL) %>% 
  pivot_wider(names_from = GENE_SYMBOL, values_from = value) %>% 
  rename(SAMPLE_ID = name) %>% # change 'name' to 'SAMPLE_ID' for clarity
  left_join(metadata_fsample %>% select(SAMPLE_ID, CONCENTRATION, REPLICATE), by = "SAMPLE_ID") %>%
  mutate(REPLICATE = as.character(REPLICATE))


# We perform pca analysis on the numerical columns (the count data)
pca_object = prcomp(pca_data %>% select(where(is.numeric)), center = F, scale. = F)

# We print the output
{  print("First 10 column of the count data")
  print(pca_data %>% head() %>% select(1:10))
  cat("\n")
  autoplot(object = pca_object, data = pca_data, colour = "CONCENTRATION", shape = "REPLICATE",  size = 2) + 
    theme_bw()
}
```

After rescaling the x and y axis of the PCA plot what conclusion can you make and does it overlap with your previous conclusion?
 That there are no further outliers and that they correspond
```{r error=F,warning=F,message=F}
# We rescale the x and y coordinates to -1 to 1 and print the new plot
autoplot(object = pca_object, data = pca_data, colour = "CONCENTRATION", shape = "REPLICATE",  size = 2) + 
  theme_bw() + coord_cartesian(xlim = c(-1,1), ylim = c(-1,1))

```


### Replicate correlation 
* What is the definition of a replicate?
 A repetition of an experiment
* Why do we analyze the correlation between replicates?
 For the validation of the study
* Do the replicates (for the same treatment condition) correlate with each other?
 Yes, its above 0.9
```{r error=F,warning=F,message=F}
# We combine the replicates from the same treatment condition and perform replicate correlation using the ggpairs function
correlation = countdata_cpm_fsample_fprobe_sumprobe %>%
  # rownames_to_column(var = "GENE_SYMBOL") %>%
  pivot_longer(-GENE_SYMBOL,names_to = "SAMPLE_ID") %>%
  left_join(metadata_fsample, by = "SAMPLE_ID") %>%
  select(GENE_SYMBOL, SAMPLE_ID, MEAN_ID, value) %>% 
  nest_by(MEAN_ID) %>% 
  mutate(data = list(data %>% pivot_wider(names_from = SAMPLE_ID, values_from = value)),
         plot = list(ggpairs(data = data %>% select(-GENE_SYMBOL),upper = list(continuous = "cor")) + theme_bw())) 

# We print the output
  for(i in 1:54){
    print(correlation$MEAN_ID[[i]])
    print(correlation$plot[[i]])
  }
  
```


#### General CPM correlation plot
 * What can you conclude from this correlation plot? Do the results overlap with your conclusions from the PCA plot?
 No, a lot does not correlate very well
 *  What conclusion can you make using this plot that you could not make using the replicate correlation plot?
 A lot correlate below 0.8 and the PCA is only the first 4 samples
```{r error=F,warning=F,message=F}
# We correlate all the count data and generate a correlation plot
plot = ggcorrplot(corr = correlate(countdata_cpm_fsample_fprobe_sumprobe, diagonal = 1, quiet = T) %>% 
                    column_to_rownames(var = "term"), lab = FALSE, hc.order = T) +
  scale_fill_gradient2(limit = c(0.8,1), low = "white", high =  "red", mid = "lightblue", midpoint = 0.9) +
   theme(axis.text.x = element_text(size = 6), axis.text.y = element_text(size = 6))


# We print the output
  plot
```


# Save output
We save our preprocessed raw count data (`countdata_raw_fsample_fprobe_sumprobe`) and metadata (`metadata_fsample`) in preperation for the DEG analysis. Since the DESeq2 package used for the DEG analysis performs its own normalization, we specifically save the raw count data rather than the normalized count data.
```{r}
# Save your countdata
write_csv(countdata_raw_fsample_fprobe_sumprobe, file.path(output_dir, paste0(gsub( "-", "", Sys.Date()), "_countdata_raw_processed.csv"))) 

# Save your countdata
write_csv(countdata_cpm_fsample_fprobe_sumprobe, file.path(output_dir, paste0(gsub( "-", "", Sys.Date()), "_countdata_cpm_processed.csv"))) 

# Save your metadata
write_csv(metadata_fsample, file.path(output_dir, paste0(gsub( "-", "", Sys.Date()), "_metadata_processed.csv"))) 
```

```{r}
install.packages("ggVennDiagram")

Genes_excluded <- list()
Genes_excluded$cisplatin <- get_low_cpm_probes_cisplatin
Genes_excluded$cyclosporine <- get_low_cpm_probes_cyclosporine
 
ggVennDiagram(Genes_excluded)

Genes_excluded <- list()
Genes_excluded$cisplatin <- 1:100
Genes_excluded$cyclosporine <- 48:150

ggVennDiagram(Genes_excluded)
```
